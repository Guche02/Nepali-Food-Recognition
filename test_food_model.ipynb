{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Testing Custom Mask R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from keras.preprocessing.image import load_img, array_to_img, img_to_array\n",
    "\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "from mrcnn.visualize import display_instances\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "\n",
    "#import custom\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = \"D:\\Youtube_MaskRCNN\"\n",
    "\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For model testing, uncomment this and load your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = \"D:\\\\Youtube_MaskRCNN\\\\mask_rcnn_object_0100.h5\"   # change it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConfig(Config):\n",
    "    \"\"\"Configuration for training on the custom  dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"object\"\n",
    "\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    NUM_CLASSES = 1 + 4  # Background + rice pudding, plain rice, lentils\n",
    "    \n",
    "    BATCH_SIZE = 4\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 78\n",
    "\n",
    "    # Skip detections with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CustomConfig()\n",
    "CUSTOM_DIR = os.path.join(ROOT_DIR, \"/dataset/\")\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.8\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(utils.Dataset):\n",
    "\n",
    "    def load_custom(self, dataset_dir, subset):\n",
    "        \"\"\"Load a subset of the Dog-Cat dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val\n",
    "        \"\"\"\n",
    "        # Add classes. We have only one class to add.\n",
    "        self.add_class(\"object\", 1, \"egg\")\n",
    "        self.add_class(\"object\", 2, \"rice\")\n",
    "        self.add_class(\"object\", 3, \"lentils\")\n",
    "        self.add_class(\"object\", 4, \"spinach\")\n",
    "\n",
    "        # Train or validation dataset?\n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # Load annotations\n",
    "        annotations1 = json.load(open(os.path.join(dataset_dir, 'via_region_data.json')))\n",
    "        annotations = list(annotations1.values())\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            image_id = \"{}_{}\".format(subset, a['filename'])\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            polygons = [r['shape_attributes'] for r in a['regions']]\n",
    "            objects = [s['region_attributes']['name'] for s in a['regions']]\n",
    "            name_dict = {\"egg\": 1, \"rice\": 2, \"lentils\": 3, \"spinach\":4}\n",
    "            num_ids = [name_dict[a] for a in objects]\n",
    "\n",
    "            self.add_image(\n",
    "                \"object\",\n",
    "                image_id=image_id,\n",
    "                path=image_path,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                polygons=polygons,\n",
    "                num_ids=num_ids\n",
    "            )\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"object\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] != \"object\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        num_ids = info['num_ids']\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "            mask[rr, cc, i] = 1\n",
    "\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\n",
    "        return mask, num_ids\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"object\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model in training or inference modes values: 'inference' or 'training'\n",
    "TEST_MODE = \"inference\"\n",
    "ROOT_DIR = \"D:\\Youtube_MaskRCNN\\dataset\"\n",
    "\n",
    "def get_ax(rows=1, cols=1, size=16):\n",
    "  \"\"\"Return a Matplotlib Axes array to be used in all visualizations in the notebook.  Provide a central point to control graph sizes. Adjust the size attribute to control how big to render images\"\"\"\n",
    "  _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "  return ax\n",
    "\n",
    "# Load validation dataset\n",
    "# Must call before using the dataset\n",
    "CUSTOM_DIR = \"D:\\Youtube_MaskRCNN\\dataset\"\n",
    "dataset_val = CustomDataset()\n",
    "dataset_val.load_custom(CUSTOM_DIR, \"val\")\n",
    "dataset_val.prepare()\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset_val.image_ids), dataset_val.class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CustomDataset()\n",
    "dataset_train.load_custom(CUSTOM_DIR, \"train\")\n",
    "dataset_train.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test the model, use the following codes:\n",
    "# To test the metrics, skip all these codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CustomConfig()\n",
    "#LOAD MODEL. Create model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir= MODEL_DIR, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO weights Or, load the last model you trained\n",
    "weights_path = WEIGHTS_PATH\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we are ready for testing our model on any image.\n",
    "class_names = dataset_val.class_names\n",
    "\n",
    "img = load_img(\"D:\\\\Youtube_MaskRCNN\\\\dataset\\lentils_plainRice_spinach_35.jpeg\")\n",
    "img_arr = np.array(img)\n",
    "results = model.detect([img_arr], verbose=1)\n",
    "r = results[0]\n",
    "visualize.display_instances(img_arr, r['rois'], r['masks'], r['class_ids'], \n",
    "                                class_names, r['scores'], figsize=(5,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing images using test folder.\n",
    "\n",
    "real_test_dir = \"D:\\Youtube_MaskRCNN\\\\friedEGG\"\n",
    "image_paths = []\n",
    "for filename in os.listdir(real_test_dir):\n",
    "    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:\n",
    "        image_paths.append(os.path.join(real_test_dir, filename))\n",
    "class_names = dataset_val.class_names\n",
    "count = 0\n",
    "for image_path in image_paths:\n",
    "    img = load_img(image_path)\n",
    "    img_arr = np.array(img)\n",
    "    results = model.detect([img_arr], verbose=1)\n",
    "    r = results[0]\n",
    "    count += 1\n",
    "    visualize.display_instances(img_arr, r['rois'], r['masks'], r['class_ids'], \n",
    "                                class_names, r['scores'], figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test the metrics, directly run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config = CustomConfig()\n",
    "\n",
    "# Define the directory where the models are stored\n",
    "model_dir = \"D:\\\\Youtube_MaskRCNN\\\\logs\\\\test\"\n",
    "\n",
    "#LOAD MODEL. Create model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=model_dir, config=config)\n",
    "\n",
    "# Define a list to store the metrics for each epoch\n",
    "epoch_metrics = []\n",
    "\n",
    "# Iterate over each epoch from 1 to 50\n",
    "for epoch in range(1, 100):\n",
    "    # Load the model for the current epoch\n",
    "    weights_path = os.path.join(model_dir, f\"mask_rcnn_object_00{epoch}.h5\")\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    # Initialize lists to store metrics for the current epoch\n",
    "    epoch_recall = []\n",
    "    epoch_precision = []\n",
    "    epoch_f1 = []\n",
    "    epoch_mAP =[]\n",
    "    \n",
    "    image_ids = dataset_val.image_ids\n",
    "\n",
    "    APs = []\n",
    "    \n",
    "    # Iterate over images\n",
    "    for image_id in image_ids:\n",
    "        # Load image and ground truth data\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset_train, config,\n",
    "                                   image_id, use_mini_mask=False)\n",
    "        molded_images = np.expand_dims(modellib.mold_image(image, config), 0)\n",
    "        \n",
    "        # Run object detection\n",
    "        results = model.detect([image], verbose=0)\n",
    "        r = results[0]\n",
    "        \n",
    "        # Compute AP\n",
    "        AP, precisions, recalls, overlaps =\\\n",
    "            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                             r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'], iou_threshold=0.5)\n",
    "        \n",
    "        APs.append(AP)\n",
    "        \n",
    "        # Append precision and recall for the current image\n",
    "        epoch_recall.append(np.mean(recalls))\n",
    "        epoch_precision.append(np.mean(precisions))\n",
    "        epoch_mAP.append(np.mean(APs))\n",
    "        \n",
    "        # Compute F1 score\n",
    "        f1_score = (2 * (np.mean(precisions) * np.mean(recalls))) / (np.mean(precisions) + np.mean(recalls))\n",
    "        epoch_f1.append(f1_score)\n",
    "    \n",
    "    # Compute average precision, recall, and F1 score for the epoch\n",
    "    avg_recall = np.mean(epoch_recall)\n",
    "    avg_precision = np.mean(epoch_precision)\n",
    "    avg_f1 = np.mean(epoch_f1)\n",
    "    avg_mAP = np.mean(epoch_mAP)\n",
    "\n",
    "    # Store the metrics for the current epoch\n",
    "    epoch_metrics.append({\"Epoch\": epoch, \"Precision\": avg_precision, \"Recall\": avg_recall, \"F1\": avg_f1, \"mAP\": avg_mAP})\n",
    "\n",
    "# Print the metrics for each epoch\n",
    "for metrics in epoch_metrics:\n",
    "    print(f\"Epoch {metrics['Epoch']}: Precision - {metrics['Precision']}, Recall - {metrics['Recall']}, F1 - {metrics['F1']}, mAP - {metrics['mAP']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "config = CustomConfig()\n",
    " \n",
    "# Define the directory where the models are stored\n",
    "model_dir = \"D:\\mask_rcnn\\logs\\object20240220T0020\"\n",
    " \n",
    "#LOAD MODEL. Create model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=model_dir, config=config)\n",
    " \n",
    "# Define a list to store the metrics for each epoch\n",
    "epoch_metrics = []\n",
    " \n",
    "# Iterate over each epoch from 1 to 100\n",
    "for epoch in range(1, 101):\n",
    "    # Load the model for the current epoch\n",
    "    weights_path = os.path.join(model_dir, f\"mask_rcnn_object_{epoch:04d}.h5\")\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    " \n",
    "    # Initialize lists to store metrics for the current epoch\n",
    "    epoch_recall = []\n",
    "    epoch_precision = []\n",
    "    epoch_f1 = []\n",
    "    epoch_mAP =[]\n",
    "    \n",
    "    image_ids = dataset_val.image_ids\n",
    "\n",
    "    APs = []\n",
    "    # Iterate over images\n",
    "    for image_id in image_ids:\n",
    "        # Load image and ground truth data\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset_train, config,\n",
    "                                   image_id, use_mini_mask=False)\n",
    "        molded_images = np.expand_dims(modellib.mold_image(image, config), 0)\n",
    "        # Run object detection\n",
    "        results = model.detect([image], verbose=0)\n",
    "        r = results[0]\n",
    "        # Compute AP\n",
    "        AP, precisions, recalls, overlaps =\\\n",
    "            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                             r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'], iou_threshold=0.5)\n",
    "        APs.append(AP)\n",
    "        # Append precision and recall for the current image\n",
    "        epoch_recall.append(np.mean(recalls))\n",
    "        epoch_precision.append(np.mean(precisions))\n",
    "        epoch_mAP.append(np.mean(APs))\n",
    "        # Compute F1 score\n",
    "        f1_score = (2 * (np.mean(precisions) * np.mean(recalls))) / (np.mean(precisions) + np.mean(recalls))\n",
    "        epoch_f1.append(f1_score)\n",
    "    # Compute average precision, recall, and F1 score for the epoch\n",
    "    avg_recall = np.mean(epoch_recall)\n",
    "    avg_precision = np.mean(epoch_precision)\n",
    "    avg_f1 = np.mean(epoch_f1)\n",
    "    avg_mAP = np.mean(epoch_mAP)\n",
    " \n",
    "    # Store the metrics for the current epoch\n",
    "    epoch_metrics.append({\"Epoch\": epoch, \"Precision\": avg_precision, \"Recall\": avg_recall, \"F1\": avg_f1, \"mAP\": avg_mAP})\n",
    " \n",
    "# Print the metrics for each epoch\n",
    "for metrics in epoch_metrics:\n",
    "    print(f\"Epoch {metrics['Epoch']}: Precision - {metrics['Precision']}, Recall - {metrics['Recall']}, F1 - {metrics['F1']}, mAP - {metrics['mAP']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data for plotting\n",
    "epochs = [metrics['Epoch'] for metrics in epoch_metrics]\n",
    "precisions = [metrics['Precision'] for metrics in epoch_metrics]\n",
    "recalls = [metrics['Recall'] for metrics in epoch_metrics]\n",
    "f1_scores = [metrics['F1'] for metrics in epoch_metrics]\n",
    "mAPs = [metrics['mAP'] for metrics in epoch_metrics]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(epochs, precisions, marker='o')\n",
    "plt.title('Epochs vs Precision')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(epochs, recalls, marker='o')\n",
    "plt.title('Epochs vs Recall')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(epochs, f1_scores, marker='o')\n",
    "plt.title('Epochs vs F1 Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(epochs, mAPs, marker='o')\n",
    "plt.title('Epochs vs mAP')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('mAP')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.75\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_train, config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'], iou_threshold=0.75)\n",
    "    APs.append(AP)\n",
    "print(\"mAP at IoU threshold 0.75: \", np.mean(APs))\n",
    "print(APs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "recall = []\n",
    "precision = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_train, config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    _, precisions, recalls, _ =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'], iou_threshold=0.75)\n",
    "    recall.append(np.mean(recalls))\n",
    "    precision.append(np.mean(precisions))\n",
    "print(\"F1: \", (2* (np.mean(precisions) * np.mean(recalls)))/(np.mean(precisions) + np.mean(recalls)))\n",
    "print(\"recall: \", np.mean(recall))\n",
    "print(\"Precision: \", np.mean(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder containing your data\n",
    "folder_path = \"D:\\\\Youtube_MaskRCNN\\\\dataset\\\\val\"\n",
    "\n",
    "# Initialize lists to store precision, recall, and AP values for all images\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_APs = []\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Assuming you load ground truth and predictions for each image, implement your loading function\n",
    "    # gt_bbox, gt_class_id, gt_mask, pred_bbox, pred_class_id, pred_scores, pred_mask = load_data_from_file(os.path.join(folder_path, filename))\n",
    "    \n",
    "    # Compute precision, recall, and AP for the current image\n",
    "    AP, precisions, recalls, _ = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                                          pred_bbox, pred_class_id, pred_scores, pred_mask)\n",
    "    \n",
    "    # Append the values to the lists\n",
    "    all_APs.append(AP)\n",
    "    all_precisions.extend(precisions)\n",
    "    all_recalls.extend(recalls)\n",
    "\n",
    "# Compute the mean average precision (mAP) over all images\n",
    "mAP = np.mean(all_APs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_id in range(1, dataset_val.num_classes):  # Assuming class IDs start from 1\n",
    "        # Check if overlaps for the current class exist\n",
    "        if class_id in overlaps:\n",
    "            visualize.plot_overlaps(gt_class_id, r['class_ids'], r['scores'], overlaps[class_id],\n",
    "                                    dataset_val.class_names[class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RPN trainig targets\n",
    "# target_rpn_match is 1 for positive anchors, -1 for negative anchors\n",
    "# and 0 for neutral anchors.\n",
    "target_rpn_match, target_rpn_bbox = modellib.build_rpn_targets(\n",
    "    image.shape, model.anchors, gt_class_id, gt_bbox, model.config)\n",
    "log(\"target_rpn_match\", target_rpn_match)\n",
    "log(\"target_rpn_bbox\", target_rpn_bbox)\n",
    "\n",
    "positive_anchor_ix = np.where(target_rpn_match[:] == 1)[0]\n",
    "negative_anchor_ix = np.where(target_rpn_match[:] == -1)[0]\n",
    "neutral_anchor_ix = np.where(target_rpn_match[:] == 0)[0]\n",
    "positive_anchors = model.anchors[positive_anchor_ix]\n",
    "negative_anchors = model.anchors[negative_anchor_ix]\n",
    "neutral_anchors = model.anchors[neutral_anchor_ix]\n",
    "log(\"positive_anchors\", positive_anchors)\n",
    "log(\"negative_anchors\", negative_anchors)\n",
    "log(\"neutral anchors\", neutral_anchors)\n",
    "\n",
    "# Apply refinement deltas to positive anchors\n",
    "refined_anchors = utils.apply_box_deltas(\n",
    "    positive_anchors,\n",
    "    target_rpn_bbox[:positive_anchors.shape[0]] * model.config.RPN_BBOX_STD_DEV)\n",
    "log(\"refined_anchors\", refined_anchors, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to get predictions for the validation dataset\n",
    "def get_predictions(model, dataset):\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(dataset.image_ids)):\n",
    "        image, _, gt_class_id, gt_bbox = modellib.load_image_gt(dataset, config, i)\n",
    "        molded_images = np.expand_dims(modellib.mold_image(image, config), 0)\n",
    "        \n",
    "        # Use batch size of 1 for predictions\n",
    "        results = model.detect([image], verbose=0)\n",
    "        r = results[0]\n",
    "\n",
    "        # Assuming that your model outputs class_ids, scores, and bounding boxes\n",
    "        predictions.append({\n",
    "            'class_ids': r['class_ids'],\n",
    "            'scores': r['scores'],\n",
    "            'rois': r['rois'],  # Bounding boxes\n",
    "        })\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Get ground truth labels and model predictions\n",
    "all_ground_truth = [modellib.load_image_gt(dataset_val, config, i) for i in range(len(dataset_val.image_ids))]\n",
    "\n",
    "# Extract true labels\n",
    "true_labels = [\n",
    "    [\n",
    "        dataset_val.class_names[label] if label != 0 else 'BG'  # Handle 'BG' class\n",
    "        for label in gt_info[2]\n",
    "    ] for gt_info in all_ground_truth\n",
    "]\n",
    "\n",
    "# Call get_predictions once for the entire dataset\n",
    "all_predictions = get_predictions(model, dataset_val)\n",
    "\n",
    "# Extract predicted labels\n",
    "predicted_labels = [\n",
    "    [\n",
    "        dataset_val.class_names[label] if label != 0 else 'BG'  # Handle 'BG' class\n",
    "        for label in p['class_ids'] if label != 0\n",
    "    ] for p in all_predictions\n",
    "]\n",
    "\n",
    "# Extract detected class IDs for each instance\n",
    "detected_class_ids = [\n",
    "    [dataset_val.class_names[label] if label != 0 else 'BG' for label in p['class_ids']]\n",
    "    for p in all_predictions\n",
    "]\n",
    "\n",
    "# Flatten the ground truth, predicted labels, and detected class IDs\n",
    "flat_true_labels = [label for sublist in true_labels for label in sublist]\n",
    "flat_predicted_labels = [label for sublist in predicted_labels for label in sublist]\n",
    "flat_detected_class_ids = [label for sublist in detected_class_ids for label in sublist]\n",
    "\n",
    "# Make sure lengths match by truncating the longer list\n",
    "min_length = min(len(flat_true_labels), len(flat_predicted_labels), len(flat_detected_class_ids))\n",
    "flat_true_labels = flat_true_labels[:min_length]\n",
    "flat_predicted_labels = flat_predicted_labels[:min_length]\n",
    "flat_detected_class_ids = flat_detected_class_ids[:min_length]\n",
    "\n",
    "# Create confusion matrix\n",
    "unique_labels = [label for label in dataset_val.class_names if label != 'BG']  # Exclude 'BG'\n",
    "cm = confusion_matrix(flat_true_labels, flat_predicted_labels, labels=unique_labels)\n",
    "\n",
    "# Visualize confusion matrix using seaborn\n",
    "plt.figure(figsize=(len(unique_labels), len(unique_labels)))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Additional: Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(\n",
    "    flat_true_labels, \n",
    "    flat_predicted_labels, \n",
    "    labels=[label for label in range(1, len(unique_labels) + 1)],  # Exclude 'BG'\n",
    "    target_names=unique_labels\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of ground truth objects and their predictions\n",
    "AP, precisions, recalls, overlaps = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                                          r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "\n",
    "visualize.plot_overlaps(gt_class_id, r['class_ids'], r['scores'],\n",
    "                        overlaps, dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display positive anchors before refinement (dotted) and\n",
    "# after refinement (solid).\n",
    "visualize.draw_boxes(image, boxes=positive_anchors, refined_boxes=refined_anchors, ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of proposals.\n",
    "# Proposals classified as background are dotted, and\n",
    "# the rest show their class and confidence score.\n",
    "limit = 200\n",
    "ixs = np.random.randint(0, proposals.shape[0], limit)\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n",
    "            for c, s in zip(roi_class_ids[ixs], roi_scores[ixs])]\n",
    "visualize.draw_boxes(image, boxes=proposals[ixs],\n",
    "                     visibilities=np.where(roi_class_ids[ixs] > 0, 2, 1),\n",
    "                     captions=captions, title=\"ROIs Before Refinement\",\n",
    "                     ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RPN sub-graph\n",
    "pillar = model.keras_model.get_layer(\"ROI\").output  # node to start searching from\n",
    "\n",
    "# TF 1.4 and 1.9 introduce new versions of NMS. Search for all names to support TF 1.3~1.10\n",
    "nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression:0\")\n",
    "if nms_node is None:\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV2:0\")\n",
    "if nms_node is None: #TF 1.9-1.10\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV3:0\")\n",
    "\n",
    "rpn = model.run_graph([image], [\n",
    "    (\"rpn_class\", model.keras_model.get_layer(\"rpn_class\").output),\n",
    "    (\"pre_nms_anchors\", model.ancestor(pillar, \"ROI/pre_nms_anchors:0\")),\n",
    "    (\"refined_anchors\", model.ancestor(pillar, \"ROI/refined_anchors:0\")),\n",
    "    (\"refined_anchors_clipped\", model.ancestor(pillar, \"ROI/refined_anchors_clipped:0\")),\n",
    "    (\"post_nms_anchor_ix\", nms_node),\n",
    "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top anchors by score (before refinement)\n",
    "limit = 100\n",
    "sorted_anchor_ids = np.argsort(rpn['rpn_class'][:,:,1].flatten())[::-1]\n",
    "visualize.draw_boxes(image, boxes=model.anchors[sorted_anchor_ids[:limit]], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top anchors with refinement. Then with clipping to image boundaries\n",
    "limit = 50\n",
    "ax = get_ax(1, 2)\n",
    "pre_nms_anchors = utils.denorm_boxes(rpn[\"pre_nms_anchors\"][0], image.shape[:2])\n",
    "refined_anchors = utils.denorm_boxes(rpn[\"refined_anchors\"][0], image.shape[:2])\n",
    "refined_anchors_clipped = utils.denorm_boxes(rpn[\"refined_anchors_clipped\"][0], image.shape[:2])\n",
    "visualize.draw_boxes(image, boxes=pre_nms_anchors[:limit],\n",
    "                     refined_boxes=refined_anchors[:limit], ax=ax[0])\n",
    "visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[:limit], ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(np.transpose(gt_mask, [2, 0, 1]), cmap=\"Blues\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maskMain",
   "language": "python",
   "name": "maskmain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
